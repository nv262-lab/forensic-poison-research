name: Run RAG Forensic Experiments (create log stores)

on:
  workflow_dispatch: {}

permissions:
  contents: read
  id-token: write

env:
  AWS_LOG_BUCKET_DEFAULT: "rag-forensic-logs-${{ github.run_id }}"
  GCP_LOG_BUCKET_DEFAULT: "rag-forensic-logs-${{ github.run_id }}"
  AZURE_LOG_RG_DEFAULT: "rag-forensic-logs-rg-${{ github.run_id }}"
  AZURE_LOG_STORAGE_DEFAULT: "ragforensiclogs${{ github.run_id }}"

jobs:
  provision-and-run:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: "3.10"

      - name: Install CLIs and libs
        run: |
          sudo apt-get update -y
          sudo apt-get install -y jq unzip
          python -m pip install --upgrade pip
          pip install boto3 google-cloud-storage azure-identity azure-mgmt-resource azure-mgmt-storage azure-storage-blob || true
          curl "https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip" -o /tmp/awscliv2.zip
          unzip -q /tmp/awscliv2.zip -d /tmp && sudo /tmp/aws/install || true
          # install gcloud if available in runner; non-fatal
          if ! command -v gcloud >/dev/null 2>&1; then
            curl -sSL https://sdk.cloud.google.com | bash > /dev/null || true
            export PATH="$HOME/google-cloud-sdk/bin:$PATH"
          fi
          sudo apt-get install -y apt-transport-https ca-certificates curl gnupg lsb-release || true
          curl -sL https://packages.microsoft.com/keys/microsoft.asc | gpg --dearmor > microsoft.gpg
          sudo mv microsoft.gpg /etc/apt/trusted.gpg.d/ || true
          AZ_REPO=$(lsb_release -cs || echo focal)
          echo "deb [arch=amd64] https://packages.microsoft.com/repos/azure-cli/ $AZ_REPO main" | sudo tee /etc/apt/sources.list.d/azure-cli.list
          sudo apt-get update || true
          sudo apt-get install -y azure-cli || true

      - name: Configure AWS creds (env)
        if: ${{ secrets.AWS_ACCESS_KEY_ID != '' }}
        env:
          AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
          AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          AWS_DEFAULT_REGION: ${{ secrets.AWS_DEFAULT_REGION }}
        run: |
          export AWS_DEFAULT_REGION=${AWS_DEFAULT_REGION}
          BUCKET="${{ secrets.AWS_LOG_BUCKET }}" || true
          if [ -z "${{ secrets.AWS_LOG_BUCKET }}" ]; then
            BUCKET="${AWS_LOG_BUCKET_DEFAULT}"
          fi
          echo "Using S3 bucket: $BUCKET"
          if aws s3api head-bucket --bucket "$BUCKET" 2>/dev/null; then
            echo "S3 bucket exists: $BUCKET"
          else
            aws s3api create-bucket --bucket "$BUCKET" --create-bucket-configuration LocationConstraint=${AWS_DEFAULT_REGION} || true
            echo "Created bucket $BUCKET"
          fi
          aws s3api put-bucket-versioning --bucket "$BUCKET" --versioning-configuration Status=Enabled || true
          TRAIL_NAME="rag-forensic-trail-${{ github.run_id }}"
          aws cloudtrail create-trail --name "$TRAIL_NAME" --s3-bucket-name "$BUCKET" || true
          aws cloudtrail put-event-selectors --trail-name "$TRAIL_NAME" --event-selectors '[{"ReadWriteType":"All","IncludeManagementEvents":true,"DataResources":[{"Type":"AWS::S3::Object","Values":["arn:aws:s3:::'"$BUCKET"'/"]}]}]' || true
          aws cloudtrail start-logging --name "$TRAIL_NAME" || true
          echo "AWS_LOG_BUCKET=$BUCKET" >> $GITHUB_ENV

      - name: Configure GCP creds & create bucket
        if: ${{ secrets.GCP_SA_KEY != '' }}
        env:
          GCP_SA_KEY: ${{ secrets.GCP_SA_KEY }}
          GCP_PROJECT: ${{ secrets.GCP_PROJECT }}
          GCP_REGION: ${{ secrets.GCP_REGION }}
        run: |
          echo "$GCP_SA_KEY" > /tmp/gcp_key.json
          export GOOGLE_APPLICATION_CREDENTIALS=/tmp/gcp_key.json
          gcloud auth activate-service-account --key-file=/tmp/gcp_key.json --project="$GCP_PROJECT" || true
          if [ -n "${{ secrets.GCP_LOG_BUCKET }}" ]; then
            BUCKET="${{ secrets.GCP_LOG_BUCKET }}"
          else
            BUCKET="${GCP_LOG_BUCKET_DEFAULT}"
          fi
          if gsutil ls -b "gs://$BUCKET" >/dev/null 2>&1; then
            echo "GCS bucket exists: $BUCKET"
          else
            gsutil mb -p "$GCP_PROJECT" -c STANDARD -l "${GCP_REGION:-us-central1}" "gs://$BUCKET"
            echo "Created GCS bucket $BUCKET"
          fi
          gsutil versioning set on "gs://$BUCKET" || true
          SINK_NAME="rag-forensic-sink-${{ github.run_id }}"
          gcloud logging sinks create "$SINK_NAME" "storage.googleapis.com/$BUCKET" --project="$GCP_PROJECT" --log-filter='resource.type="gcs_bucket" OR protoPayload.methodName:("storage.objects.insert" OR "storage.objects.update")' || true
          echo "GCP_LOG_BUCKET=$BUCKET" >> $GITHUB_ENV

      - name: Configure Azure and create storage container
        if: ${{ secrets.AZURE_CLIENT_ID != '' }}
        env:
          AZURE_CLIENT_ID: ${{ secrets.AZURE_CLIENT_ID }}
          AZURE_CLIENT_SECRET: ${{ secrets.AZURE_CLIENT_SECRET }}
          AZURE_TENANT_ID: ${{ secrets.AZURE_TENANT_ID }}
          AZURE_SUBSCRIPTION_ID: ${{ secrets.AZURE_SUBSCRIPTION_ID }}
        run: |
          az login --service-principal -u "$AZURE_CLIENT_ID" -p "$AZURE_CLIENT_SECRET" --tenant "$AZURE_TENANT_ID" >/dev/null
          az account set --subscription "$AZURE_SUBSCRIPTION_ID"
          if [ -n "${{ secrets.AZURE_LOG_RG }}" ]; then
            RG="${{ secrets.AZURE_LOG_RG }}"
          else
            RG="${AZURE_LOG_RG_DEFAULT}"
          fi
          if [ -n "${{ secrets.AZURE_LOG_STORAGE }}" ]; then
            STORAGE_NAME="${{ secrets.AZURE_LOG_STORAGE }}"
          else
            STORAGE_NAME="${AZURE_LOG_STORAGE_DEFAULT}"
          fi
          CONTAINER="rag-logs"
          if az group show -n "$RG" >/dev/null 2>&1; then
            echo "RG exists: $RG"
          else
            az group create -n "$RG" -l eastus || true
            echo "Created RG $RG"
          fi
          if az storage account check-name --name "$STORAGE_NAME" --query 'nameAvailable' | grep true >/dev/null 2>&1; then
            az storage account create -n "$STORAGE_NAME" -g "$RG" --sku Standard_LRS || true
            echo "Created storage account $STORAGE_NAME"
          else
            echo "Storage account may already exist: $STORAGE_NAME"
          fi
          CONN=$(az storage account show-connection-string -n "$STORAGE_NAME" -g "$RG" --query connectionString -o tsv)
          echo "AZURE_STORAGE_CONNECTION_STRING=$CONN" >> $GITHUB_ENV
          az storage container create --name "$CONTAINER" --connection-string "$CONN" || true
          az storage blob service-properties update --enable-versioning true --account-name "$STORAGE_NAME" || true
          echo "AZURE_LOG_CONTAINER=$CONTAINER" >> $GITHUB_ENV
          echo "AZURE_LOG_STORAGE=$STORAGE_NAME" >> $GITHUB_ENV
          echo "AZURE_LOG_RG=$RG" >> $GITHUB_ENV

      - name: Export RAG public key
        env:
          RAG_RSA_PUB: ${{ secrets.RAG_RSA_PUB }}
        run: |
          if [ -n "$RAG_RSA_PUB" ]; then
            echo "$RAG_RSA_PUB" > /tmp/rag_rsa_pub.pem
            chmod 600 /tmp/rag_rsa_pub.pem
          fi

      - name: Generate demo keys if missing
        run: |
          python3 tools/keygen_demo.py || true

      - name: Install python deps for repo
        run: |
          pip install -r requirements.txt || true

      - name: Seed corpus
        run: |
          python3 src/rag/data/seed_generator.py

      - name: Build local index
        run: |
          python3 -c "import json; docs=[_import_('json').loads(l) for l in open('src/rag/data/corpus/corpus.jsonl')]; from src.rag.vectorstore.faiss_store import FaissStore; FaissStore('data/faiss_index').build(docs)"

      - name: Run scenarios
        run: |
          python3 src/rag/experiments/runner.py --scenarios all --count 1

      - name: Collect exported cloud logs (download from created stores)
        run: |
          mkdir -p data/logs
          # AWS S3 fetch
          if [ -n "${AWS_LOG_BUCKET:-}" ]; then
            python3 tools/collect_cloud_logs.py --provider aws --bucket "${AWS_LOG_BUCKET}" --prefix "" --out data/logs/aws || true
          fi
          # GCP fetch
          if [ -n "${GCP_LOG_BUCKET:-}" ]; then
            python3 tools/collect_cloud_logs.py --provider gcp --bucket "${GCP_LOG_BUCKET}" --prefix "" --out data/logs/gcp || true
          fi
          # Azure fetch
          if [ -n "${AZURE_LOG_CONTAINER:-}" ]; then
            export AZURE_STORAGE_CONNECTION_STRING="${AZURE_STORAGE_CONNECTION_STRING}"
            python3 tools/collect_cloud_logs.py --provider azure --container "${AZURE_LOG_CONTAINER}" --prefix "" --out data/logs/azure || true
          fi

      - name: Run detection pass & emit SIEM events
        run: |
          python3 -c "from src.rag.detectors.run_all import run_all; run_all(output='data/backups/siem_events.json')" || true

      - name: Upload artifacts
        uses: actions/upload-artifact@v4
        with:
          name: rag-experiment-output
          path: |
            data/faiss_index/**
            data/backups/**
            data/logs/**

      - name: Show SIEM events
        run: |
          if [ -f data/backups/siem_events.json ]; then jq -C . data/backups/siem_events.json || true; else echo "no siem events"; fi
