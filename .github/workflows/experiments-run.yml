name: Run RAG Forensic Experiments (create log stores)

on:
  workflow_dispatch: {}

permissions:
  contents: read
  id-token: write

env:
  AWS_LOG_BUCKET_DEFAULT: "rag-forensic-logs-${{ github.run_id }}"
  GCP_LOG_BUCKET_DEFAULT: "rag-forensic-logs-${{ github.run_id }}"
  AZURE_LOG_RG_DEFAULT: "rag-forensic-logs-rg-${{ github.run_id }}"
  AZURE_LOG_STORAGE_DEFAULT: "ragforensiclogs${{ github.run_id }}"
  TF_BACKEND_DIR: infrastructure/terraform/backend
  TF_ROOT_DIR: infrastructure/terraform

jobs:
  provision-and-run:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Setup Terraform
        uses: hashicorp/setup-terraform@v2
        with:
          terraform_version: "1.5.7"
          terraform_wrapper: true

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: "3.11"

      - name: Export repo root and src to PYTHONPATH (persisted for later steps)
        run: |
          echo "PYTHONPATH=$PWD:src" >> $GITHUB_ENV

      - name: Install CLIs and libs
        run: |
          sudo apt-get update -y
          sudo apt-get install -y jq unzip
          python -m pip install --upgrade pip
          pip install boto3 google-cloud-storage azure-identity azure-mgmt-resource azure-mgmt-storage azure-storage-blob || true
          curl "https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip" -o /tmp/awscliv2.zip
          unzip -q /tmp/awscliv2.zip -d /tmp && sudo /tmp/aws/install || true
          if ! command -v gcloud >/dev/null 2>&1; then
            curl -sSL https://sdk.cloud.google.com | bash > /dev/null || true
            export PATH="$HOME/google-cloud-sdk/bin:$PATH"
          fi
          sudo apt-get install -y apt-transport-https ca-certificates curl gnupg lsb-release || true
          curl -sL https://packages.microsoft.com/keys/microsoft.asc | gpg --dearmor > microsoft.gpg || true
          sudo mv microsoft.gpg /etc/apt/trusted.gpg.d/ || true
          AZ_REPO=$(lsb_release -cs || echo focal)
          echo "deb [arch=amd64] https://packages.microsoft.com/repos/azure-cli/ $AZ_REPO main" | sudo tee /etc/apt/sources.list.d/azure-cli.list
          sudo apt-get update || true
          sudo apt-get install -y azure-cli || true

      - name: Install python deps and package
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt || true
          pip install -e . || true

      - name: Generate demo keys if missing
        run: python3 tools/keygen_demo.py || true

      - name: Run unit tests
        run: |
          export PYTHONPATH="$PWD:src"
          pytest -q src/rag/tests || true

      - name: Debug Terraform init
        if: always()
        run: |
          terraform -version || true
          ls -la || true
          terraform init -input=false -upgrade -lockfile=auto || true
          cat .terraform.lock.hcl || true

      - name: Make setup script executable
        run: chmod +x .github/scripts/setup_cloud_logs.sh || true

      # ------------------------
      # Authenticate to clouds BEFORE running terraform backend workspace
      # ------------------------

      - name: Configure AWS credentials (action)
        uses: aws-actions/configure-aws-credentials@v2
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-session-token: ${{ secrets.AWS_SESSION_TOKEN || '' }}
          aws-region: ${{ secrets.AWS_DEFAULT_REGION }}

      - name: Validate AWS identity
        run: aws sts get-caller-identity || true

      - name: Setup gcloud (action)
        uses: google-github-actions/setup-gcloud@v1
        with:
          service_account_key: ${{ secrets.GCP_SA_KEY || '' }}
          project_id: ${{ secrets.GCP_PROJECT || '' }}
          export_default_credentials: true

      - name: Login to Azure (action)
        uses: azure/login@v1
        with:
          creds: |
            {
              "clientId": "${{ secrets.AZURE_CLIENT_ID }}",
              "clientSecret": "${{ secrets.AZURE_CLIENT_SECRET }}",
              "tenantId": "${{ secrets.AZURE_TENANT_ID }}",
              "subscriptionId": "${{ secrets.AZURE_SUBSCRIPTION_ID }}"
            }

      - name: Validate Azure account
        run: |
          az account show --query "{id:id, user:user.name}" -o json || true
          az login --service-principal -u ${{ secrets.AZURE_CLIENT_ID }} -p ${{ secrets.AZURE_CLIENT_SECRET }} --tenant ${{ secrets.AZURE_TENANT_ID }} || true

      - name: Export TF_VARs for backend workspace
        run: |
          echo "TF_VAR_aws_region=${{ secrets.AWS_DEFAULT_REGION }}" >> $GITHUB_ENV
          echo "TF_VAR_gcp_project=${{ secrets.GCP_PROJECT }}" >> $GITHUB_ENV
          echo "TF_VAR_gcp_region=${{ secrets.GCP_REGION || '' }}" >> $GITHUB_ENV
          echo "TF_VAR_azure_rg_name=${{ secrets.BACKEND_AZURE_RG || '' }}" >> $GITHUB_ENV
          echo "TF_VAR_azure_storage_account=${{ secrets.BACKEND_AZURE_STORAGE_ACCOUNT || '' }}" >> $GITHUB_ENV
          echo "TF_VAR_tf_state_bucket=${{ secrets.BACKEND_TF_STATE_BUCKET || '' }}" >> $GITHUB_ENV
          echo "TF_VAR_tf_lock_table=${{ secrets.BACKEND_TF_LOCK_TABLE || '' }}" >> $GITHUB_ENV
          echo "TF_VAR_gcs_bucket=${{ secrets.BACKEND_GCS_BUCKET || '' }}" >> $GITHUB_ENV
          echo "TF_VAR_create_aws=${{ secrets.BACKEND_CREATE_AWS || 'true' }}" >> $GITHUB_ENV
          echo "TF_VAR_create_gcp=${{ secrets.BACKEND_CREATE_GCP || 'false' }}" >> $GITHUB_ENV
          echo "TF_VAR_create_azure=${{ secrets.BACKEND_CREATE_AZURE || 'false' }}" >> $GITHUB_ENV
          echo "TF_VAR_azure_client_id=${{ secrets.AZURE_CLIENT_ID }}" >> $GITHUB_ENV
          echo "TF_VAR_azure_client_secret=${{ secrets.AZURE_CLIENT_SECRET }}" >> $GITHUB_ENV
          echo "TF_VAR_azure_tenant_id=${{ secrets.AZURE_TENANT_ID }}" >> $GITHUB_ENV
          echo "TF_VAR_azure_subscription_id =${{ secrets.AZURE_SUBSCRIPTION_ID }}" >> $GITHUB_ENV

      # ------------------------
      # Existing per-cloud log store creation steps (use repo secrets and backend outputs)
      # ------------------------

      - name: Configure AWS creds (env)
        env:
          AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
          AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          AWS_DEFAULT_REGION: ${{ secrets.AWS_DEFAULT_REGION }}
          AWS_LOG_BUCKET: ${{ secrets.AWS_LOG_BUCKET || '' }}
        run: |
          existing_trails=$(aws cloudtrail describe-trails --query 'trailList[*].Name' --output text || true)
          if [ "$(echo "$existing_trails" | wc -w)" -ge 5 ]; then
            echo "Maximum number of trails reached. Existing trails: $existing_trails"
            echo "Skipping creation of a new trail."
          else
            TRAIL_NAME="rag-forensic-trail-${{ github.run_id }}"
            aws cloudtrail create-trail --name "$TRAIL_NAME" --s3-bucket-name "${AWS_LOG_BUCKET:-$AWS_LOG_BUCKET_DEFAULT}" || true
            aws cloudtrail put-event-selectors --trail-name "$TRAIL_NAME" --event-selectors '[{"ReadWriteType":"All","IncludeManagementEvents":true,"DataResources":[{"Type":"AWS::S3::Object","Values":["arn:aws:s3:::'"${AWS_LOG_BUCKET:-$AWS_LOG_BUCKET_DEFAULT}"'/"]}]}]' || true
            aws cloudtrail start-logging --name "$TRAIL_NAME" || true
          fi

      - name: Configure GCP creds & create bucket
        env:
          GCP_SA_KEY: ${{ secrets.GCP_SA_KEY || '' }}
          GCP_PROJECT: ${{ secrets.GCP_PROJECT || '' }}
          GCP_REGION: ${{ secrets.GCP_REGION || '' }}
          GCP_LOG_BUCKET: ${{ secrets.GCP_LOG_BUCKET || '' }}
          GCP_ACCESS_TOKEN: ${{ secrets.gcp_access_token || '' }}
        run: |
          if [ -n "${GCP_SA_KEY:-}" ]; then
            echo "$GCP_SA_KEY" > /tmp/gcp_key.json
            export GOOGLE_APPLICATION_CREDENTIALS=/tmp/gcp_key.json
            gcloud auth login
            gcloud auth activate-service-account --key-file=/tmp/gcp_key.json --project="${{ secrets.GCP_PROJECT }}" || true
          fi

          if [ -n "${GCP_LOG_BUCKET}" ]; then
            BUCKET="${GCP_LOG_BUCKET}"
          else
            BUCKET="${GCP_LOG_BUCKET_DEFAULT}"
          fi

          if gsutil ls -b "gs://$BUCKET" >/dev/null 2>&1; then
            echo "GCS bucket exists: $BUCKET"
          else
            gsutil mb -p "${{ secrets.GCP_PROJECT }}" -c STANDARD -l "${{ secrets.GCP_REGION || 'us-central1' }}" "gs://$BUCKET" || true
            echo "Created GCS bucket $BUCKET"
          fi
          gsutil versioning set on "gs://$BUCKET" || true
          SINK_NAME="rag-forensic-sink-${{ github.run_id }}"
          gcloud logging sinks create "$SINK_NAME" "storage.googleapis.com/$BUCKET" --project="${{ secrets.GCP_PROJECT }}" --log-filter='resource.type="gcs_bucket" OR protoPayload.methodName:("storage.objects.insert" OR "storage.objects.update")' || true
          echo "GCP_LOG_BUCKET=$BUCKET" >> $GITHUB_ENV

      - name: Configure Azure and create storage container
        env:
          AZURE_CLIENT_ID: ${{ secrets.AZURE_CLIENT_ID }}
          AZURE_CLIENT_SECRET: ${{ secrets.AZURE_CLIENT_SECRET }}
          AZURE_TENANT_ID: ${{ secrets.AZURE_TENANT_ID }}
          AZURE_SUBSCRIPTION_ID: ${{ secrets.AZURE_SUBSCRIPTION_ID }}
          AZURE_LOG_RG: ${{ secrets.AZURE_LOG_RG || '' }}
          AZURE_LOG_STORAGE: ${{ secrets.AZURE_LOG_STORAGE || '' }}
        run: |
          az login --service-principal -u "${{ secrets.AZURE_CLIENT_ID }}" -p "${{ secrets.AZURE_CLIENT_SECRET }}" --tenant "${{ secrets.AZURE_TENANT_ID }}" >/dev/null || true
          az account set --subscription "${{ secrets.AZURE_SUBSCRIPTION_ID }}" || true

          RG="${AZURE_LOG_RG:-$AZURE_LOG_RG_DEFAULT}"
          STORAGE_NAME="${AZURE_LOG_STORAGE:-$AZURE_LOG_STORAGE_DEFAULT}"
          STORAGE_NAME=$(echo "$STORAGE_NAME" | tr '[:upper:]' '[:lower:]' | tr -d '[:punct:]' | cut -c1-24)
          CONTAINER="rag-logs"

          if az group show -n "$RG" >/dev/null 2>&1; then
            echo "Resource Group exists: $RG"
          else
            az group create -n "$RG" -l eastus || true
            echo "Created Resource Group $RG"
          fi

          if az storage account show -n "$STORAGE_NAME" -g "$RG" >/dev/null 2>&1; then
            echo "Storage account already exists: $STORAGE_NAME"
          else
            az storage account create -n "$STORAGE_NAME" -g "$RG" --sku Standard_LRS || true
            echo "Created storage account $STORAGE_NAME"
          fi

          CONN=$(az storage account show-connection-string -n "$STORAGE_NAME" -g "$RG" --query connectionString -o tsv)
          echo "AZURE_STORAGE_CONNECTION_STRING=$CONN" >> $GITHUB_ENV
          az storage container create --name "$CONTAINER" --connection-string "$CONN" || true
          az storage blob service-properties update --enable-versioning true --account-name "$STORAGE_NAME" || true
          echo "AZURE_LOG_CONTAINER=$CONTAINER" >> $GITHUB_ENV
          echo "AZURE_LOG_STORAGE=$STORAGE_NAME" >> $GITHUB_ENV
          echo "AZURE_LOG_RG=$RG" >> $GITHUB_ENV

      - name: Export RAG public key
        env:
          RAG_RSA_PUB: ${{ secrets.RAG_RSA_PUB || '' }}
        run: |
          if [ -n "${RAG_RSA_PUB}" ]; then
            echo "${RAG_RSA_PUB}" > /tmp/rag_rsa_pub.pem
            chmod 600 /tmp/rag_rsa_pub.pem
          fi

      - name: Seed corpus
        run: python3 src/rag/data/seed_generator.py || true

      - name: Build local index
        run: |
          python3 -c "
          import json
          from src.rag.vectorstore.faiss_store import FaissStore
          with open('src/rag/data/corpus/corpus.jsonl') as f:
              docs = [json.loads(line) for line in f]
          FaissStore('data/faiss_index').build(docs)
          " || true

      - name: Run scenarios
        run: python3 src/rag/experiments/runner.py --scenarios all --count 1 || true

      - name: Collect exported cloud logs (download from created stores)
        env:
          GCP_ACCESS_TOKEN: ${{ secrets.gcp_access_token || '' }}
        run: |
          mkdir -p data/logs
          # AWS
          if [ -n "${AWS_LOG_BUCKET:-}" ]; then
            python3 tools/collect_cloud_logs.py --provider aws --bucket "${AWS_LOG_BUCKET}" --prefix "" --out data/logs/aws || true
          elif [ -n "${aws_tf_state_bucket:-}" ]; then
            python3 tools/collect_cloud_logs.py --provider aws --bucket "${aws_tf_state_bucket}" --prefix "" --out data/logs/aws || true
          else
            echo "No AWS log bucket configured"
          fi

          # GCP (use access token if available)
          if [ -n "${GCP_LOG_BUCKET:-}" ]; then
            python3 tools/collect_cloud_logs.py --provider gcp --bucket "${GCP_LOG_BUCKET}" --prefix "" --out data/logs/gcp --gcp-access-token "${GCP_ACCESS_TOKEN:-}" || true
          elif [ -n "${gcp_gcs_bucket:-}" ]; then
            python3 tools/collect_cloud_logs.py --provider gcp --bucket "${gcp_gcs_bucket}" --prefix "" --out data/logs/gcp --gcp-access-token "${GCP_ACCESS_TOKEN:-}" || true
          else
            echo "No GCP log bucket configured"
          fi

          # Azure
          if [ -n "${AZURE_LOG_CONTAINER:-}" ]; then
            export AZURE_STORAGE_CONNECTION_STRING="${AZURE_STORAGE_CONNECTION_STRING:-$AZURE_STORAGE_CONNECTION_STRING}"
            python3 tools/collect_cloud_logs.py --provider azure --container "${AZURE_LOG_CONTAINER}" --prefix "" --out data/logs/azure || true
          elif [ -n "${azure_container_name:-}" ]; then
            export AZURE_STORAGE_CONNECTION_STRING="${AZURE_STORAGE_CONNECTION_STRING:-$AZURE_STORAGE_CONNECTION_STRING}"
            python3 tools/collect_cloud_logs.py --provider azure --container "${azure_container_name}" --prefix "" --out data/logs/azure || true
          else
            echo "No Azure log container configured"
          fi

      - name: List collected logs
        run: |
          echo "Collected files:"
          find data/logs -type f -maxdepth 5 -print || true

      - name: Run detection pass & emit SIEM events
        run: |
          python3 -c "import sys; sys.path.insert(0,'src'); from rag.detectors.run_all import run_all; run_all(output='data/backups/siem_events.json')" || true

      - name: Upload artifacts
        uses: actions/upload-artifact@v4
        with:
          name: rag-experiment-output
          path: |
            data/faiss_index/**
            data/backups/**
            data/logs/**

      - name: Show SIEM events
        run: |
          if [ -f data/backups/siem_events.json ]; then jq -C . data/backups/siem_events.json || true; else echo "no siem events"; fi
