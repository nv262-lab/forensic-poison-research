name: Run RAG Forensic Experiments (create log stores)

on:
  workflow_dispatch: {}

permissions:
  contents: read
  id-token: write

env:
  AWS_LOG_BUCKET_DEFAULT: "rag-forensic-logs-${{ github.run_id }}"
  GCP_LOG_BUCKET_DEFAULT: "rag-forensic-logs-${{ github.run_id }}"
  AZURE_LOG_RG_DEFAULT: "rag-forensic-logs-rg-${{ github.run_id }}"
  AZURE_LOG_STORAGE_DEFAULT: "ragforensiclogs${{ github.run_id }}"
  TF_BACKEND_DIR: infrastructure/terraform/backend
  TF_ROOT_DIR: infrastructure/terraform

jobs:
  provision-and-run:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Setup Terraform
        uses: hashicorp/setup-terraform@v2
        with:
          terraform_version: "1.5.7"
          terraform_wrapper: true

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: "3.11"

      - name: Export repo root to PYTHONPATH (persisted for later steps)
        run: echo "PYTHONPATH=$PWD" >> $GITHUB_ENV

      - name: Install CLIs and libs
        run: |
          sudo apt-get update -y
          sudo apt-get install -y jq unzip
          python -m pip install --upgrade pip
          pip install boto3 google-cloud-storage azure-identity azure-mgmt-resource azure-mgmt-storage azure-storage-blob || true
          curl "https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip" -o /tmp/awscliv2.zip
          unzip -q /tmp/awscliv2.zip -d /tmp && sudo /tmp/aws/install || true
          if ! command -v gcloud >/dev/null 2>&1; then
            curl -sSL https://sdk.cloud.google.com | bash > /dev/null || true
            export PATH="$HOME/google-cloud-sdk/bin:$PATH"
          fi
          sudo apt-get install -y apt-transport-https ca-certificates curl gnupg lsb-release || true
          curl -sL https://packages.microsoft.com/keys/microsoft.asc | gpg --dearmor > microsoft.gpg || true
          sudo mv microsoft.gpg /etc/apt/trusted.gpg.d/ || true
          AZ_REPO=$(lsb_release -cs || echo focal)
          echo "deb [arch=amd64] https://packages.microsoft.com/repos/azure-cli/ $AZ_REPO main" | sudo tee /etc/apt/sources.list.d/azure-cli.list
          sudo apt-get update || true
          sudo apt-get install -y azure-cli || true

      - name: Install python deps and package
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt || true
          pip install -e . || true

      - name: Generate demo keys if missing
        run: python3 tools/keygen_demo.py || true

      - name: Run unit tests
        run: |
          export PYTHONPATH="$PWD"
          pytest -q src/rag/tests

      - name: Debug Terraform init
        if: always()
        run: |
          terraform -version || true
          ls -la || true
          terraform init -input=false -upgrade -lockfile=auto || true
          cat .terraform.lock.hcl || true

      - name: Make setup script executable
        run: chmod +x .github/scripts/setup_cloud_logs.sh

      # ------------------------
      # Authenticate to clouds BEFORE running terraform backend workspace
      # ------------------------

      - name: Configure AWS credentials (action)
        uses: aws-actions/configure-aws-credentials@v2
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-session-token: ${{ secrets.AWS_SESSION_TOKEN || '' }}
          aws-region: ${{ secrets.AWS_DEFAULT_REGION }}

      - name: Validate AWS identity
        run: aws sts get-caller-identity

      - name: Setup gcloud (action)
        uses: google-github-actions/setup-gcloud@v1
        with:
          service_account_key: ${{ secrets.GCP_SA_KEY }}
          project_id: ${{ secrets.GCP_PROJECT }}
          export_default_credentials: true

      - name: Export GOOGLE_APPLICATION_CREDENTIALS
        if: ${{ secrets.GCP_SA_KEY != '' }}
        run: |
          echo "${{ secrets.GCP_SA_KEY }}" > /tmp/gcp_key.json
          chmod 600 /tmp/gcp_key.json
          echo "GOOGLE_APPLICATION_CREDENTIALS=/tmp/gcp_key.json" >> $GITHUB_ENV

      - name: Validate GCP identity
        run: gcloud auth list --format="value(account)" || true

      - name: Login to Azure (action)
        uses: azure/login@v1
        with:
          creds: |
            {
              "clientId": "${{ secrets.AZURE_CLIENT_ID }}",
              "clientSecret": "${{ secrets.AZURE_CLIENT_SECRET }}",
              "tenantId": "${{ secrets.AZURE_TENANT_ID }}",
              "subscriptionId": "${{ secrets.AZURE_SUBSCRIPTION_ID }}"
            }

      - name: Validate Azure account
        run: az account show --query "{id:id, user:user.name}" -o json

      # Export TF_VARs for backend apply (from repository secrets)
      - name: Export TF_VARs for backend workspace
        run: |
          echo "TF_VAR_aws_region=${{ secrets.AWS_DEFAULT_REGION }}" >> $GITHUB_ENV
          echo "TF_VAR_gcp_project=${{ secrets.GCP_PROJECT }}" >> $GITHUB_ENV
          echo "TF_VAR_gcp_region=${{ secrets.GCP_REGION || '' }}" >> $GITHUB_ENV
          echo "TF_VAR_azure_rg_name=${{ secrets.BACKEND_AZURE_RG || '' }}" >> $GITHUB_ENV
          echo "TF_VAR_azure_storage_account=${{ secrets.BACKEND_AZURE_STORAGE_ACCOUNT || '' }}" >> $GITHUB_ENV
          echo "TF_VAR_tf_state_bucket=${{ secrets.BACKEND_TF_STATE_BUCKET || '' }}" >> $GITHUB_ENV
          echo "TF_VAR_tf_lock_table=${{ secrets.BACKEND_TF_LOCK_TABLE || '' }}" >> $GITHUB_ENV
          echo "TF_VAR_gcs_bucket=${{ secrets.BACKEND_GCS_BUCKET || '' }}" >> $GITHUB_ENV
          echo "TF_VAR_create_aws=${{ secrets.BACKEND_CREATE_AWS || 'true' }}" >> $GITHUB_ENV
          echo "TF_VAR_create_gcp=${{ secrets.BACKEND_CREATE_GCP || 'false' }}" >> $GITHUB_ENV
          echo "TF_VAR_create_azure=${{ secrets.BACKEND_CREATE_AZURE || 'false' }}" >> $GITHUB_ENV

      # ------------------------
      # Run backend workspace (create remote backend resources)
      # ------------------------
      - name: Backend - Terraform Init
        working-directory: ${{ env.TF_BACKEND_DIR }}
        run: terraform init -input=false

      - name: Backend - Terraform Apply
        working-directory: ${{ env.TF_BACKEND_DIR }}
        run: terraform apply -auto-approve -input=false

      - name: Capture backend outputs
        id: backend_outputs
        working-directory: ${{ env.TF_BACKEND_DIR }}
        run: terraform output -json > tf_backend_outputs.json && cat tf_backend_outputs.json

      - name: Export backend outputs to env
        working-directory: ${{ env.TF_BACKEND_DIR }}
        run: |
          if [ -f tf_backend_outputs.json ]; then
            jq -r 'to_entries[] | "ยง.key)=@customBackSlash(.value.value)"' tf_backend_outputs.json > backend_envs.sh@customNewLine            # transform to export lines@customNewLine            sed -n 's/^@customBackSlash(.!ยง=ยง.!ยง$/export \1="\2"/p' backend_envs.sh > backend_envs_export.sh || true
            cat backend_envs_export.sh
            echo "BACKEND_OUTPUTS=backend_envs_export.sh" >> $GITHUB_ENV
          fi

      - name: Load backend outputs into environment
        if: always()
        run: |
          if [ -f backend_envs_export.sh ]; then
            set -a
            . ./backend_envs_export.sh
            set +a
          fi

      # ------------------------
      # Existing per-cloud log store creation steps (use repo secrets and backend outputs)
      # ------------------------

      - name: Configure AWS creds (env)
        env:
          AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
          AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          AWS_DEFAULT_REGION: ${{ secrets.AWS_DEFAULT_REGION }}
          AWS_LOG_BUCKET: ${{ secrets.AWS_LOG_BUCKET || '' }}
        run: |
          .github/scripts/setup_cloud_logs.sh
          export AWS_DEFAULT_REGION=${{ secrets.AWS_DEFAULT_REGION }}
          if [ -n "${AWS_LOG_BUCKET}" ]; then
            BUCKET="${AWS_LOG_BUCKET}"
          elif [ -n "${aws_tf_state_bucket:-}" ]; then
            BUCKET="${aws_tf_state_bucket}"
          else
            BUCKET="${AWS_LOG_BUCKET_DEFAULT}"
          fi
          echo "Using S3 bucket: $BUCKET"
          if aws s3api head-bucket --bucket "$BUCKET" 2>/dev/null; then
            echo "S3 bucket exists: $BUCKET"
          else
            aws s3api create-bucket --bucket "$BUCKET" --create-bucket-configuration LocationConstraint=${{ secrets.AWS_DEFAULT_REGION }} || true
            echo "Created bucket $BUCKET"
          fi
          aws s3api put-bucket-versioning --bucket "$BUCKET" --versioning-configuration Status=Enabled || true
          TRAIL_NAME="rag-forensic-trail-${{ github.run_id }}"
          aws cloudtrail create-trail --name "$TRAIL_NAME" --s3-bucket-name "$BUCKET" || true
          aws cloudtrail put-event-selectors --trail-name "$TRAIL_NAME" --event-selectors '[{"ReadWriteType":"All","IncludeManagementEvents":true,"DataResources":[{"Type":"AWS::S3::Object","Values":["arn:aws:s3:::'"$BUCKET"'/"]}]}]' || true
          aws cloudtrail start-logging --name "$TRAIL_NAME" || true
          echo "AWS_LOG_BUCKET=$BUCKET" >> $GITHUB_ENV

      - name: Configure GCP creds & create bucket
        env:
          GCP_SA_KEY: ${{ secrets.GCP_SA_KEY }}
          GCP_PROJECT: ${{ secrets.GCP_PROJECT }}
          GCP_REGION: ${{ secrets.GCP_REGION || '' }}
          GCP_LOG_BUCKET: ${{ secrets.GCP_LOG_BUCKET || '' }}
        run: |
          echo "$GCP_SA_KEY" > /tmp/gcp_key.json
          export GOOGLE_APPLICATION_CREDENTIALS=/tmp/gcp_key.json
          gcloud auth activate-service-account --key-file=/tmp/gcp_key.json --project="${{ secrets.GCP_PROJECT }}" || true
          if [ -n "${GCP_LOG_BUCKET}" ]; then
            BUCKET="${GCP_LOG_BUCKET}"
          elif [ -n "${gcp_gcs_bucket:-}" ]; then
            BUCKET="${gcp_gcs_bucket}"
          else
            BUCKET="${GCP_LOG_BUCKET_DEFAULT}"
          fi
          if gsutil ls -b "gs://$BUCKET" >/dev/null 2>&1; then
            echo "GCS bucket exists: $BUCKET"
          else
            gsutil mb -p "${{ secrets.GCP_PROJECT }}" -c STANDARD -l "${{ secrets.GCP_REGION || 'us-central1' }}" "gs://$BUCKET"
            echo "Created GCS bucket $BUCKET"
          fi
          gsutil versioning set on "gs://$BUCKET" || true
          SINK_NAME="rag-forensic-sink-${{ github.run_id }}"
          gcloud logging sinks create "$SINK_NAME" "storage.googleapis.com/$BUCKET" --project="${{ secrets.GCP_PROJECT }}" --log-filter='resource.type="gcs_bucket" OR protoPayload.methodName:("storage.objects.insert" OR "storage.objects.update")' || true
          echo "GCP_LOG_BUCKET=$BUCKET" >> $GITHUB_ENV

      - name: Configure Azure and create storage container
        env:
          AZURE_CLIENT_ID: ${{ secrets.AZURE_CLIENT_ID }}
          AZURE_CLIENT_SECRET: ${{ secrets.AZURE_CLIENT_SECRET }}
          AZURE_TENANT_ID: ${{ secrets.AZURE_TENANT_ID }}
          AZURE_SUBSCRIPTION_ID: ${{ secrets.AZURE_SUBSCRIPTION_ID }}
          AZURE_LOG_RG: ${{ secrets.AZURE_LOG_RG || '' }}
          AZURE_LOG_STORAGE: ${{ secrets.AZURE_LOG_STORAGE || '' }}
        run: |
          az login --service-principal -u "${{ secrets.AZURE_CLIENT_ID }}" -p "${{ secrets.AZURE_CLIENT_SECRET }}" --tenant "${{ secrets.AZURE_TENANT_ID }}" >/dev/null
          az account set --subscription "${{ secrets.AZURE_SUBSCRIPTION_ID }}"
          if [ -n "${AZURE_LOG_RG}" ]; then
            RG="${AZURE_LOG_RG}"
          else
            RG="${AZURE_LOG_RG_DEFAULT}"
          fi
          if [ -n "${AZURE_LOG_STORAGE}" ]; then
            STORAGE_NAME="${AZURE_LOG_STORAGE}"
          elif [ -n "${azure_storage_account:-}" ]; then
            STORAGE_NAME="${azure_storage_account}"
          else
            STORAGE_NAME="${AZURE_LOG_STORAGE_DEFAULT}"
          fi
          CONTAINER="rag-logs"
          if az group show -n "$RG" >/dev/null 2>&1; then
            echo "RG exists: $RG"
          else
            az group create -n "$RG" -l eastus || true
            echo "Created RG $RG"
          fi
          if az storage account check-name --name "$STORAGE_NAME" --query 'nameAvailable' | grep true >/dev/null 2>&1; then
            az storage account create -n "$STORAGE_NAME" -g "$RG" --sku Standard_LRS || true
            echo "Created storage account $STORAGE_NAME"
          else
            echo "Storage account may already exist: $STORAGE_NAME"
          fi
          CONN=$(az storage account show-connection-string -n "$STORAGE_NAME" -g "$RG" --query connectionString -o tsv)
          echo "AZURE_STORAGE_CONNECTION_STRING=$CONN" >> $GITHUB_ENV
          az storage container create --name "$CONTAINER" --connection-string "$CONN" || true
          az storage blob service-properties update --enable-versioning true --account-name "$STORAGE_NAME" || true
          echo "AZURE_LOG_CONTAINER=$CONTAINER" >> $GITHUB_ENV
          echo "AZURE_LOG_STORAGE=$STORAGE_NAME" >> $GITHUB_ENV
          echo "AZURE_LOG_RG=$RG" >> $GITHUB_ENV

      - name: Export RAG public key
        env:
          RAG_RSA_PUB: ${{ secrets.RAG_RSA_PUB || '' }}
        run: |
          if [ -n "${RAG_RSA_PUB}" ]; then
            echo "${RAG_RSA_PUB}" > /tmp/rag_rsa_pub.pem
            chmod 600 /tmp/rag_rsa_pub.pem
          fi

      - name: Seed corpus
        run: python3 src/rag/data/seed_generator.py

      - name: Build local index
        run: |
          python3 -c "import json; docs=[import('json').loads(l) for l in open('src/rag/data/corpus/corpus.jsonl')]; from src.rag.vectorstore.faiss_store import FaissStore; FaissStore('data/faiss_index').build(docs)"

      - name: Run scenarios
        run: python3 src/rag/experiments/runner.py --scenarios all --count 1

      - name: Collect exported cloud logs (download from created stores)
        run: |
          mkdir -p data/logs
          if [ -n "${AWS_LOG_BUCKET:-}" ]; then
            python3 tools/collect_cloud_logs.py --provider aws --bucket "${AWS_LOG_BUCKET}" --prefix "" --out data/logs/aws || true
          elif [ -n "${aws_tf_state_bucket:-}" ]; then
            python3 tools/collect_cloud_logs.py --provider aws --bucket "${aws_tf_state_bucket}" --prefix "" --out data/logs/aws || true
          fi
          if [ -n "${GCP_LOG_BUCKET:-}" ]; then
            python3 tools/collect_cloud_logs.py --provider gcp --bucket "${GCP_LOG_BUCKET}" --prefix "" --out data/logs/gcp || true
          elif [ -n "${gcp_gcs_bucket:-}" ]; then
            python3 tools/collect_cloud_logs.py --provider gcp --bucket "${gcp_gcs_bucket}" --prefix "" --out data/logs/gcp || true
          fi
          if [ -n "${AZURE_LOG_CONTAINER:-}" ]; then
            export AZURE_STORAGE_CONNECTION_STRING="${AZURE_STORAGE_CONNECTION_STRING}"
            python3 tools/collect_cloud_logs.py --provider azure --container "${AZURE_LOG_CONTAINER}" --prefix "" --out data/logs/azure || true
          elif [ -n "${azure_container_name:-}" ]; then
            export AZURE_STORAGE_CONNECTION_STRING="${AZURE_STORAGE_CONNECTION_STRING}"
            python3 tools/collect_cloud_logs.py --provider azure --container "${azure_container_name}" --prefix "" --out data/logs/azure || true
          fi

      - name: Run detection pass & emit SIEM events
        run: python3 -c "from src.rag.detectors.run_all import run_all; run_all(output='data/backups/siem_events.json')" || true

      - name: Upload artifacts
        uses: actions/upload-artifact@v4
        with:
          name: rag-experiment-output
          path: |
            data/faiss_index/**
            data/backups/**
            data/logs/**

      - name: Show SIEM events
        run: |
          if [ -f data/backups/siem_events.json ]; then jq -C . data/backups/siem_events.json || true; else echo "no siem events"; fi
